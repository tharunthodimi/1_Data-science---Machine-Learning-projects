{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color= \"green\"> This is Compressed Folder please execute all the cells below to check the results of EDA and model Building results in your local machine </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Overview:\n",
    "\n",
    "- Import Libraries\n",
    "- Data Description and null value check\n",
    "- Univariate Analysis\n",
    "- Null value Treatment and Outlier Treatment\n",
    "- Applying Logistic Regression model\n",
    "- Statistical Analysis\n",
    "- ROC Curve\n",
    "- Thresold value Identification- Accuracy,sensitivity and specificty curve\n",
    "- validation Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Reading the Leads.csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read csv and store in data and  display top 5 rows of the dataframe\n",
    "data=pd.read_csv('Leads.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "- We have 9240 row sand 37 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display non-null,datatype of eacch feature\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "\n",
    "- Majority of the columns are in object dattatype and we could see they are as expected.Since the columns/features are actually categorical datatype is Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical summary  of the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- We have very less number of features with integer/Float datatype so only 7 features has been displyed in the statistical summary\n",
    "- By observing the count we could see there are null values present in the data. we will handle them in further steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable method for plotting count plot for multiple features\n",
    "def uni_anal(x,rotationvalue):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title('Distribution of Catgeories in '+ x+ ' Feature',size=15,color='Green')\n",
    "    sns.countplot(x=x,hue='Converted',data=data)\n",
    "    plt.xticks(rotation=rotationvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uni_anal('Lead Origin',45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- Lead Origin indiactes the origin identifier with which the customer was identified to be a lead.\n",
    "- Among the 5 categories of Lead Origin `Landing Api Submission` and `API` plays a key role\n",
    "- `Lead Import` and `Quick Add Form` has a very less count on the scale lets treat them in further steps.since the catgeories are skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('Lead Source',90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Lead Source` tells us about the source of the lead from which lead has entered the details.\n",
    "- When we consider the count of both converted and Non-Converted we could see the highest to lowest sources are in the following order: `Google`,`Direct Traffic`, `Ohark Chat`, `Organic Search` and few others.\n",
    "- We could see lot of sources through which the lead source has been obtained. We need to handle the data since data is skewed with very less count for some of the sources.We need to handle this else we will have moree categories which can affect the model perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('Do Not Email',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Do Not Email` tells about the lead intention that whether he/she want to be emailed about the course or not.\n",
    "- We have only two categories here and the count of `NO` ismore when compared with the count of `YES` which tells us that the majority has selected `NO` indicates that there are high count of people intrested in receiving the mails about the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('Do Not Call',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets understadn the count of Yes category as well \n",
    "data['Do Not Call'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Do Not Call` tells about the lead intention that whether he/she want to be called about the course or not.\n",
    "- We have only two categories here and the count of `NO` is more when compared with the count of `YES` which tells us that the majority has selected `NO` indicates that there are high count of people intrested in receiving the calls about the course.\n",
    "- We can drop this column in further steps since the feature is saying the same information for all the rows and it will not make any difference since all the users intention is same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('What matters most to you in choosing a course',45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['What matters most to you in choosing a course'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `What matters most to you in choosing a course` tells about the customers main motto behind doing this course.\n",
    "- We have only three categories here and the count of `Better Career Prospects` is more when compared with the count of other categories adn we could see the other category count is very less which is equal to 3.\n",
    "- And lot of null values are already present in this feature.We can drop this column since it tells the same information for all the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('Update me on Supply Chain Content',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Update me on Supply Chain Content'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Update me on Supply Chain Content` tells about the intention of the customer to recieve updates on the Supply Chain Content.\n",
    "- We have only one category here which is `NO` and we can drop this feature since this feature provide information to the entire data.SO we can ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets analyse the special category columns which Indicates whether the customer had seen the ad in any of the listed items and Through Recommendations features.\n",
    "AdVisitedIn=['Search', 'Magazine','Newspaper Article', 'X Education Forums', 'Newspaper','Digital Advertisement', 'Through Recommendations']\n",
    "AdVisitedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Visualise the spread of different categories\n",
    "counter=range(1,len(AdVisitedIn)+1)\n",
    "plt.figure(figsize=(30,45))\n",
    "for i,j in zip(AdVisitedIn,counter):\n",
    "    plt.subplot(4,2,j)\n",
    "    plt.title('Distribution of Catgeories in '+ i+ ' Feature',size=25,color='Green')\n",
    "    sns.countplot(x=i,hue='Converted',data=data)\n",
    "    plt.xticks(rotation=45,size=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see the spread of categories in the above features\n",
    "for i in AdVisitedIn:\n",
    "    print(data[i].value_counts())\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- We could see that `Magazine` has only one category in it so we can drop this feature in further steps since it tells the same information to all the data points,Which can be ignore to increase model perfomance.\n",
    "- We observed that the above features `Search`,`Newpaper Article`, `Education Forums`, `Newspaper`,`Digital Advertisment` and `Through Recommendations` is explaining two catgeories but the count of single catgeory is dominating heavily so we can drop these features in further steps,Since it tells the same information for all the data so we can ignore to increase perfomance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('Receive More Updates About Our Courses',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Receive More Updates About Our Courses` tells about the customer who would like to receive more updates about the courses.\n",
    " - We have only one category here which is `NO` and we can drop this feature since this feature provide same information to the entire data.SO we can ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('Tags',90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Tags` indicates about the Tags assigned to customers indicating the current status of the lead.\n",
    "- Actually this column is not filled by the user and marketing team might filled the details and this may not help in our anlysis.\n",
    "- We have customers who has converted as Hot Lead when this `Tags` feature has this category:`Will revert adter reading the email` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('Get updates on DM Content',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Get updates on DM Content` tells about the customer who wish to get the updates on the DM Content.\n",
    " - We have only one category here which is `NO` and we can drop this feature since this feature provide same information to the entire data.So we can ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('Specialization',90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Specialization` tells about the industry domain in which the customer worked before.\n",
    "- We have `Select` class at higher count so we need to treat this columns since user has not selected his/her option in this feature.\n",
    "- `Finance Management`,`Marketing Management` and `Human Resource Management` are top three categories in this feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('How did you hear about X Education',90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `How did you hear about X Education` tells about the source from which the customer heard about X Education.\n",
    "- We have `Select` class at higher count so we need to treat this columns since user has not selected his/her option in this feature.\n",
    "- `Online Search`,`Student of someschool` and `Word of mouth` are top three categories in this feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('What is your current occupation',45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `What is your current occupation` tells about whether the customer is a student, umemployed or employed.\n",
    "- We have `Unemployed` class at higher count.Followed by `Working professional and `Student`.we need to treat this columns since data is skewed because three categories consumes most of the data  in this feature we will treat to avoid skewness of the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('Last Activity',90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Last Activity` tells about whether the Last activity performed by the customer. \n",
    "- We have the following order from highest to lowest occuring categories : `Email opened`, `Sms Sent` and `Olark Chat Converstaion` followed by many other categories.\n",
    "- Data is skewed since we hae many categories but 3 categories occupies majority of the data. So we need to treat this feature since categories are skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('Lead Profile',45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Lead Profile` tells about the lead level assigned to each customer based on their profile.\n",
    "- We have multiple catgeories here indicating the highest to lowest appeared categories in the following order: `Potential Lead`, `Other Leads` and `Later Student`.\n",
    "- We should treat this column since there is lot of skewness in the data because 3 columns count is very high when compared with all other catgeories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('Country',90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Country` tells about the country of the customer.\n",
    "- We have multiple countrues to which the customers belongs to but India has high dominating values when compared with other counties.\n",
    "- We should drop this column since there is lot of skewness in the data because only one columns count is very high when compared with all other catgeories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('City',45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `City` feature tells about the city to which the city of the customer belongs to.\n",
    "- We could see special category called `Select` it is appearing because the user has not selcted the city he/she belong to.We will treat this category in furthe steps.\n",
    "- `Mumbai` has the higher count among the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## An index and score assigned to each customer based on their activity and their profile\n",
    "\n",
    "score_Assigned=['Asymmetrique Activity Index','Asymmetrique Profile Index', 'Asymmetrique Activity Score','Asymmetrique Profile Score']\n",
    "score_Assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=range(1,len(score_Assigned)+1)\n",
    "plt.figure(figsize=(30,35))\n",
    "for i,j in zip(score_Assigned,counter):\n",
    "    plt.subplot(2,2,j)\n",
    "    plt.title('Distribution of Catgeories in '+ i+ ' Feature',size=25,color='Green')\n",
    "    sns.countplot(x=i,hue='Converted',data=data)\n",
    "    plt.xticks(rotation=45,size=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- We could see that `Asymmetrique Activity Index` and `Asymmetrique Profile Index` has similar behavior indicating highest to lowest order :Medium,High and Low.\n",
    "- `Asymmetrique Activity Score` and `Asymmetrique Profile score` hs numerical data which we will consider further we could data is skewed in all the four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('I agree to pay the amount through cheque',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `I agree to pay the amount through cheque` tells about the customer who has agreed to pay the amount through cheque or not.\n",
    "- We have only one category here which is `NO` and we can drop this feature since this feature provide same information to the entire data.So we can ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('A free copy of Mastering The Interview',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `A free copy of Mastering The Interview` tells about the Indicates whether the customer wants a free copy of 'Mastering the Interview' or not.\n",
    "- We have only two categories here and the count of `NO` is more when compared with the count of `YES` which tells us that the majority has selected `NO` indicates that there are high count of people intrested in a free copy of `Mastering the Interview`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_anal('Last Notable Activity',90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Last Notable Activity` tells about the last notable acitivity performed by the student.\n",
    "- We have multiple catgeories here indicating the highest to lowest appeared categories in the following order: `Modified`, `Email Opened` and `Sms Sent`.\n",
    "- We should treat this column since there is lot of skewness in the data because 3 columns count is very high when compared with all other catgeories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Data Analysis and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuable method for univariate distribution for numerical features\n",
    "def univ_dist(x):\n",
    "    plt.figure(figsize=(20,7))\n",
    "    plt.subplot(1,2,1)    \n",
    "    plt.title('Distribution of the '+x+' feature',color='Green',size=15)\n",
    "    sns.distplot(data[x])\n",
    "    plt.xlabel(x,color='red',size=12)    \n",
    "    plt.ylabel('Density',color='red',size=12) \n",
    "    \n",
    "    plt.subplot(1,2,2)    \n",
    "    plt.title('Distribution of the percentile values for '+x+' feature',color='Green',size=15)\n",
    "    sns.boxplot(data[x])\n",
    "    plt.xlabel(x,color='red',size=12) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the distribution of TotalVisits feature\n",
    "univ_dist('TotalVisits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `TotalVisits` feature is right skewed lets see the percentile values and treat this feature.\n",
    "- We will treat this column so that skewness will not affect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univ_dist('Total Time Spent on Website')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Total Time Spent on Website` feature is right skewed lets see the percentile values and treat this feature.\n",
    "- We will treat this column so that skewness will not affect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univ_dist('Page Views Per Visit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- `Page views per visit` feature is right skewed lets see the percentile values and treat this feature.\n",
    "- We will treat this column so that skewness will not affect the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets treat the Numerical features and applying capping for outlier treatment if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets consider these features and store them in the split1 list\n",
    "split1=['TotalVisits','Total Time Spent on Website','Page Views Per Visit']\n",
    "len_split1=len(split1)\n",
    "\n",
    "print('Percentile values before capping the outliers\\n')\n",
    "for i,j in zip(split1,range(len_split1)):\n",
    "    print('Percentile values before capping the outliers for '+ i +' is :\\n', data[i].quantile([0,0.05,0.1,0.9,0.95,0.99,1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- we could see that all the feature `TotalVisits`,` total time spent on website` and `Page views per visit` has huge difference between 99th and 100th perentile values.\n",
    "- Due to this data is skewed we need to treat them so that skewness will be reduced.lets cap the outliers in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Treatment using Capping technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets consider these features and store them in the split1 list\n",
    "split1=['TotalVisits','Total Time Spent on Website','Page Views Per Visit']\n",
    "len_split1=len(split1)\n",
    "\n",
    "for i,j in zip(split1,range(len_split1)):\n",
    "    percentilevalues = data[i].quantile([0.05,0.99]).values\n",
    "    data[i] = np.clip(data[i], percentilevalues[0], percentilevalues[1])  # Replace the original features after capping the data in the original dataframe \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- As shown above we have capped the data at 5th and 99th percentile for the following features: `TotalVisits`,` total time spent on website` and `Page views per visit` has huge difference between 99th and 100th perentile values.\n",
    "- Due to this data skewness will be reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets verify the percentile values after capping the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1=['TotalVisits','Total Time Spent on Website','Page Views Per Visit']\n",
    "len_split1=len(split1)\n",
    "\n",
    "print('Percentile values After capping the outliers\\n')\n",
    "for i,j in zip(split1,range(len_split1)):\n",
    "    print('percentile values After applying capping for '+ i +' is :\\n', data[i].quantile([0,0.05,0.1,0.9,0.95,0.99,1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- As shown above we have capped the data at 5th and 99th percentile for the following features: `TotalVisits`,` total time spent on website` and `Page views per visit` has huge difference between 99th and 100th perentile values.\n",
    "- Due to this data skewness will be reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Data/Feature Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop prospect id and Lead Number since  its random number assigned to lead and doesnt look meaningful in the model\n",
    "data.drop(['Prospect ID','Lead Number'],axis=1,inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets consider categorical columns. we will drop column which have same value for  all the rows since it will not make difference in the outcome or prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the single category features\n",
    "single_category=[]\n",
    "for i in data.columns:\n",
    "    if(len(data[i].value_counts().index)==1):\n",
    "        single_category.append(i)\n",
    "print(\"\\n The count of single category columns is {0}  \\n Feature/Column names are:\".format(len(single_category)))\n",
    "print(single_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- As observed we have 5 features in which we have single catgeiry present in the data we can drop this feature. Since it will not make difference in the outcome considering this may affect the model's perfomance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the multiple category features\n",
    "multiple_category=[]\n",
    "for i in data.columns:\n",
    "    if(len(data[i].value_counts().index)>1):\n",
    "        multiple_category.append(i)\n",
    "print(\"\\n The count of multiple category is {0} \\n Feature/colum names are:\".format(len(multiple_category)))\n",
    "print(multiple_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- We have 30 features after the basice data validation lets look for further anlysis steps to treat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the single catgeory columns\n",
    "data.drop(single_category,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the shape after dropping the columns in theprevious step\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- We dropped the single catgeory columns in the above step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat the Advertised catgeory columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Advertisedcolumns=['Search','Newspaper Article','X Education Forums','Newspaper','Digital Advertisement']\n",
    "Advertisedcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Advertisedcolumns:\n",
    "    print(data[i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- We already discussed the above category and it can affect the model perfomance since all the features are giving the smae information or biased towards a single category so lets drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasedcolumns=['Search','Newspaper Article','X Education Forums','Newspaper','Digital Advertisement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(biasedcolumns,axis=1,inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "` We have dropped the 5 columns which was not worth considering for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null value check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of null values\n",
    "percentageofnullvalues=round(data.isnull().sum()/data.shape[0]*100,2)\n",
    "percentageofnullvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null value Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop null values where percentage of null values is more than 30 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 'Tags','Lead Quality','Asymmetrique Activity Index','Asymmetrique Profile Index','Asymmetrique Activity Score','Asymmetrique Profile Score'\n",
    "# dropping columns having null value percentage is gretare than 30 percent\n",
    "\n",
    "data.drop(['Tags','Lead Quality','Asymmetrique Activity Index','Asymmetrique Profile Index','Asymmetrique Activity Score','Asymmetrique Profile Score'],axis=1,inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "- We have dropped features where null value percentage is more than 30 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets verify the null value percentage again and treat them\n",
    "percentageofnullvalues=round(data.isnull().sum()/data.shape[0]*100,2)\n",
    "percentageofnullvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets treat other features as well where we have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets treat missing value of Lead Source column and replace with maximum catgeory occured\n",
    "data['Lead Source'].replace(np.NaN,data['Lead Source'].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value of Total Visits is repplace with median value \n",
    "data['TotalVisits'].replace(np.NaN,data['TotalVisits'].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value of 'Page Views Per Visit' is repplaced with median value \n",
    "data['Page Views Per Visit'].replace(np.NaN,data['Page Views Per Visit'].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace nan with most appeared category so use mode\n",
    "data['Last Activity'].replace(np.NaN,data['Last Activity'].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets understand the distribution and skewness of the Lead Source data which we observed in visualization and treat them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts of different catgeoris in Lead source feature\n",
    "data['Lead Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method which helps in replacing the values in Lead source column\n",
    "def leadsource(x):\n",
    "    if(x == 'Google'):\n",
    "        return 'Google'\n",
    "    elif x=='Direct Traffic':\n",
    "        return 'Direct Traffic'\n",
    "    elif x=='Olark Chat':\n",
    "        return 'Olark Chat'\n",
    "    elif x=='Organic Search':\n",
    "        return 'Organic Search'\n",
    "    elif x=='Reference':\n",
    "        return 'Reference'\n",
    "    elif x=='Welingak Website':\n",
    "        return 'Welingak Website'\n",
    "    elif x=='Referral Sites':\n",
    "        return 'Referral Sites'\n",
    "    else:\n",
    "        return 'Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the skewed data for Lead source column\n",
    "data['Lead Source']=data['Lead Source'].apply(leadsource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the value counts to check that the skewness has been removed\n",
    "data['Lead Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify the null value percentage\n",
    "percentageofnullvalues=round(data.isnull().sum()/data.shape[0]*100,2)\n",
    "percentageofnullvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop country and city column.\n",
    "data.drop(['Country','City'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights and Observations:\n",
    "- We have dropped country column since we observed in visualisation where majority of the rows has country as india and eventhough we try to replace  26 percent of null values with mode/Highest occuring value this percentage will increase.so its better not to consider this feature in our analysis.\n",
    "- We have dropped city column since city column has huge number of `select` category and has less infomration and may not be useful for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets verify the value_counts of multiple features\n",
    "for i in data:\n",
    "    print(data[i].value_counts())\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### As observed following features have `select` category : Specialization,How did you hear about X Education, Lead Profile. Lets treat this category since the user didn't fill the details in this category may be due to multiple reasons like user is not applicable to this category or he dont want to disclose details,Lets replace Slect category with nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace select category with nan values\n",
    "data['Specialization'].replace('Select',np.NaN,inplace=True)\n",
    "data['How did you hear about X Education'].replace('Select',np.NaN,inplace=True)\n",
    "data['Lead Profile'].replace('Select',np.NaN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets verify the nan value count again\n",
    "percentageofnullvalues=round(data.isnull().sum()/data.shape[0]*100,2)\n",
    "percentageofnullvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Features where null value percentage is more than 30 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newfeatures holds column names where column has less than 30 percent null values\n",
    "newfeatures=percentageofnullvalues[percentageofnullvalues<30].index\n",
    "newfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conider new features after dropping columns where null value count is more\n",
    "data=data[newfeatures]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets verify the nan value count again\n",
    "\n",
    "percentageofnullvalues=round(data.isnull().sum()/data.shape[0]*100,2)\n",
    "percentageofnullvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    print(data[i].value_counts())\n",
    "    print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets verify the nan value count again\n",
    "\n",
    "percentageofnullvalues=round(data.isnull().sum()/data.shape[0]*100,2)\n",
    "percentageofnullvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets consider Analysis on `What matters most to you in choosing a course`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['What matters most to you in choosing a course'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- In the above step we saw that `What matters most to you in choosing a course` has only one category which is highly dominating other two categories which are jsut 3 in number so its better to drop since the count of other category is very less to consider in model\n",
    "- Anyhow we can drop this column since the feature is saying same indformation to all the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('What matters most to you in choosing a course',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets verify the nan value count again\n",
    "\n",
    "percentageofnullvalues=round(data.isnull().sum()/data.shape[0]*100,2)\n",
    "percentageofnullvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets treat `What is your current occupation` feature since we observed the feature is skewed in visualization lets treat this featre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['What is your current occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method helps in replacing the unnecessary categories to avoid skewness\n",
    "def occupation(x):\n",
    "    if x=='Unemployed':\n",
    "        return 'Unemployed'\n",
    "    elif x=='Working Professional':\n",
    "        return 'Working Professional'    \n",
    "    else:\n",
    "        return 'Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['What is your current occupation']=data['What is your current occupation'].apply(occupation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['What is your current occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- As observed the Categorical skewness has been reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets verify the nan value count again\n",
    "\n",
    "percentageofnullvalues=round(data.isnull().sum()/data.shape[0]*100,2)\n",
    "percentageofnullvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- We are good with data cleaning lets go ahead and fit the model and perform the statistical analysis on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets split the feature names where we have null values and non null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnullvalues=[]\n",
    "nullvalues=[]\n",
    "for i in data:\n",
    "    if(data[i].isnull().sum()==0):\n",
    "        nonnullvalues.append(i) \n",
    "    else:\n",
    "        nullvalues.append(i)\n",
    "print(nonnullvalues)\n",
    "print(len(nonnullvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nullvalues)\n",
    "print(len(nullvalues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- As shown in te above steps we just splitted the data into `nullvalues` and `nonnullvalues`.\n",
    "- we dont have any `nullvalues` in the data. Lets build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering non -null values \n",
    "data=data[nonnullvalues]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add the target column to variable y\n",
    "y=data['Converted']\n",
    "y=y.astype('int')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input features are passed into x variable\n",
    "x=data.drop('Converted',axis=1)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets treat categorical and noncategorical seperately then we will combine both of the dataframes using concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non categorical features\n",
    "split1=data[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']]\n",
    "split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data of split1\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "mm=MinMaxScaler()\n",
    "sc=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1=pd.DataFrame(mm.fit_transform(split1),columns=split1.columns)\n",
    "split1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Insights:\n",
    "    - In the previous step we have applied minmax scaling on the split1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical columns as different dataframe\n",
    "split2=x.drop(['TotalVisits','Total Time Spent on Website','Page Views Per Visit'],axis=1)\n",
    "split2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- we are conisdering the categorical data into split2 variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply get_dummies on categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying get dummies on the categorical data\n",
    "split2=pd.get_dummies(split2,drop_first=True)\n",
    "split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the split1 and split2 which hold the categorical data nd integer data and stire in x2 variable\n",
    "x2=pd.concat([split1,split2],axis=1)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- As shown above get_dummies has been applied on the data and minmax scaling has been done and input is stored in x2 and output/Target is stored in y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Logistic Regression on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score,recall_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x2,y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted value on test data into y_pred\n",
    "y_pred=lr.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at classification report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- We have Accuracy at 0.83.\n",
    "- Lets do statitistical analysis and remove the statistical insignificance features in any then we will see the metrics called precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical significance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Recursive feature elimination \n",
    "from sklearn.feature_selection import RFE\n",
    "rfe=RFE(lr,15)\n",
    "rfe=rfe.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support for all the features were given here\n",
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names,Suppport and ranking \n",
    "list(zip(x_train.columns,rfe.support_,rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the column names which have high support \n",
    "columns = x_train.columns[rfe.support_]\n",
    "columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight:\n",
    "- As shown above we got the Top 15  features using recursive feature elimination.\n",
    "- Lets understand the significance of these features in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sm = sm.add_constant(x_train[columns])  # add constant since stats model requires explicitly constant declaration\n",
    "logm2 = sm.GLM(y_train,x_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = x_train[columns].columns\n",
    "vif['VIF'] = [variance_inflation_factor(x_train[columns].values, i) for i in range(x_train[columns].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- We could see that `Lead Source_Welingak Website` has P- value at 0.999 which shows that this feature is not statistical significant for the model. Lets drop this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the columns by drooping 'Lead Source_Welingak Website' and store the features in columns2\n",
    "columns2=columns.drop('Lead Source_Welingak Website')\n",
    "columns2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats model2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant since the features has changed now \n",
    "x_train_sm2 = sm.add_constant(x_train[columns2])\n",
    "logm2 = sm.GLM(y_train,x_train_sm2, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = x_train[columns2].columns\n",
    "vif['VIF'] = [variance_inflation_factor(x_train[columns2].values, i) for i in range(x_train[columns2].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- The p values are less than 0.05 except for all values\n",
    "- Considering the p-values in the above step and VIF(variance inflation factor) are as expected. And we can perform prediction on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction value on training dataset\n",
    "y_train_pred=res.predict(x_train_sm2)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the training pred variable\n",
    "y_train_pred=y_train_pred.values.reshape(-1)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- Converted `y_train_pred` to n-dimesional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- Above step shows the Conversion probability value we can use this for further analysis in finding the lead score analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final['Preditedvalue']=y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- Above step shows the converted value,Converted probability and Predicted value at cutoff 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC function\n",
    "def roccurve( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5)) # display figure size\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)'%auc_score )\n",
    "    plt.plot([0, 1], [0, 1], '--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the values at False Positive Rate,True positive rate and thresold value\n",
    "fpr,tpr,thresholds = metrics.roc_curve(y_train_pred_final.Converted,y_train_pred_final.Conversion_Prob,drop_intermediate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the roc curve\n",
    "roccurve(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating columns with different probability cutoffs to understand the prediction value at different cutoff's\n",
    "probabilities = [float(x)/10 for x in range(10)]\n",
    "for i in probabilities:\n",
    "    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- In the above step we have obtained Conversion values at different cutoff values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe to see the values of accuracy, sensitivity, and specificity at different values of probabiity cutoffs\n",
    "cutoff = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "# Forming confusion matrix to find values of sensitivity, accuracy and specificity at each level of probablity value\n",
    "from sklearn.metrics import confusion_matrix\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n",
    "    total=sum(sum(cm1))    #total vale\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total  #accuracy\n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])  #specificity\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])  #sensitivity\n",
    "    cutoff.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the curve to find the optimal value\n",
    "cutoff.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- We could observe that optimal value is around 0.35 lets consider this and make predictions in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the final_predicted value after applying the optimal value\n",
    "y_train_pred_final['final_predicted'] = y_train_pred_final.Conversion_Prob.map( lambda x: 1 if x > 0.35 else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- We have obtained the final predited value by considering the optimal value at 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionmatrix=confusion_matrix(y_train_pred_final.Converted,y_train_pred_final.final_predicted)\n",
    "confusionmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusionmatrix[1,1] # true positive \n",
    "TN = confusionmatrix[0,0] # true negatives\n",
    "FP = confusionmatrix[0,1] # false positives\n",
    "FN = confusionmatrix[1,0] # false negatives\n",
    "\n",
    "print(\"True positive: \",TP)\n",
    "print(\"True Neagtive: \",TN)\n",
    "print(\"False positive: \",FP)\n",
    "print(\"False Negative: \",FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision\n",
    "precision=TP/(TP+FP)\n",
    "print(\"Precision value is:\",precision)\n",
    "\n",
    "#Recall\n",
    "Recall=TP/(TP+FN)\n",
    "print(\"Recall value is:\",Recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- We have ontained the Precision value at 72%\n",
    "- We have obtained Recall value at 81%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the columns we got using RFE(recursive feature elimination)\n",
    "x_test=x_test[columns2]\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding constant to the test data \n",
    "x_test_sm = sm.add_constant(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = res.predict(x_test_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_pred to a dataframe which was Pandas series\n",
    "y_pred_1 = pd.DataFrame(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the head\n",
    "y_pred_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_test to dataframe\n",
    "y_test_df = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting CustID to index\n",
    "y_test_df['CustID'] = y_test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing index for both dataframes to append them side by side \n",
    "y_pred_1.reset_index(drop=True, inplace=True)\n",
    "y_test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending y_test_df and y_pred_1\n",
    "y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column \n",
    "y_pred_final= y_pred_final.rename(columns={ 0 : 'Conversion_Prob'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging the columns\n",
    "y_pred_final = y_pred_final[['CustID','Converted','Conversion_Prob']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the head of y_pred_final\n",
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the optimum threshold\n",
    "y_pred_final['final_predicted'] = y_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.35 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Accuracy, Precision and Recall on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix\n",
    "cm3 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted)\n",
    "plt.subplots(figsize=(10,10))\n",
    "ax = sns.heatmap(cm3,annot=True,cmap='coolwarm',fmt='d')\n",
    "ax.set_title('Prediction on testing data using logistic regression',fontsize=18)\n",
    "ax.set_xticklabels(['Predicted Not Converted','Predicted Converted'],fontsize=15)\n",
    "ax.set_yticklabels(['Actual Not Converted','Actual Converted'],fontsize=15)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accuracy on test data\n",
    "acc = round(metrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted),2)*100\n",
    "print(\"Accuracy is {}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = round(recall_score(y_pred_final.Converted, y_pred_final.final_predicted),2)*100\n",
    "print(\"Recall is {}%\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "\n",
    "#### `Training Data Recall value(in percentage): 81 %`\n",
    "#### `Testing Data Recall value(in percentage): 83 %`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- Our main is to increase Recall value since we need to identify the Hot leads.\n",
    "- We have good Recall value and now our model is able to predict higher Customers who are converting as Hot Leads\n",
    "- We dont have much difference in Recall value from Train and Test. Which indicates good perfomance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights:\n",
    "- Model has performed well in both training and Test data and doesn't make much difference between train and Test data\n",
    "- Since we have adjusted probability values in this approach when business startegy changes we can tune our model easily to obtain the perfomance.\n",
    "- We have concentrated more on recall and it is achieved.Since we want to identify more Leads from this model.\n",
    "- These are the Top features which is making the customers to convert as a Hot Lead:\n",
    "    - Last Notable Activity_Had a Phone Conversation\n",
    "    - Lead Origin_Lead Add Form\n",
    "    - Total Visits and  Total Time spent on Website\n",
    "    - Page Views per Visit\n",
    "    - Last Activity: SMS Sent\n",
    "    - What is your current occupation: unemployed,working professional"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
